{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@codetrade/grad-cam-in-pytorch-a-powerful-tool-for-visualize-explanations-from-deep-networks-bdc7caf0b282\n",
    "#https://www.codetrade.io/blog/grad-cam-a-complete-guide-with-example/\n",
    "#https://towardsdatascience.com/understand-your-algorithm-with-grad-cam-d3b62fce353\n",
    "#https://github.com/jacobgil/pytorch-grad-cam\n",
    "#https://arxiv.org/pdf/1610.02391\n",
    "#https://github.com/mrdbourke/pytorch-deep-learning/blob/main/03_pytorch_computer_vision.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FashionMnistModel(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_units, output_shape):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                        out_channels=hidden_units,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units,\n",
    "                        hidden_units,\n",
    "                        3,\n",
    "                        padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units,\n",
    "                        hidden_units,\n",
    "                        3,\n",
    "                        padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*7*7,\n",
    "                        out_features=output_shape),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FashionMnistModel(1, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'epochs = 5\\n\\n\\nfor epoch in range(epochs):\\n    print(f\"Epoch {epoch+1}\\n-------------------------------\")\\n    train_loss = 0\\n\\n    for batch, (X, y) in enumerate(train_dataloader):\\n        model.train()\\n        y_pred = model(X)\\n        loss = loss_fn(y_pred, y)\\n        train_loss += loss\\n\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        if batch % 100 == 0:\\n            print(f\\'looked at {batch * len(X)} / {len(train_dataloader.dataset)}\\')\\n\\n    train_loss /= len(train_dataloader)\\n\\n    test_loss, test_acc = 0, 0\\n    model.eval()\\n\\n    with torch.inference_mode():\\n        for X, y in test_dataloader:\\n            test_pred = model(X)\\n            test_loss += loss_fn(test_pred, y)\\n            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\\n        test_loss /= len(test_dataloader)\\n        test_acc /= len(test_dataloader)\\n\\n    print(f\"Train Error: {train_loss:.4f}, Test Error: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\\n\\ntest_acc = int(round(test_acc, 0))\\ntorch.save(model.state_dict(), f\"models/model_acc_{test_acc}.pth\")'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''epochs = 5\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'looked at {batch * len(X)} / {len(train_dataloader.dataset)}')\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            test_pred = model(X)\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    print(f\"Train Error: {train_loss:.4f}, Test Error: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "test_acc = int(round(test_acc, 0))\n",
    "torch.save(model.state_dict(), f\"models/model_acc_{test_acc}.pth\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8716/3261369366.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"models/model_acc_{86}.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FashionMnistModel(1, 10, 10)\n",
    "model.load_state_dict(torch.load(f\"models/model_acc_{86}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 28, 28]             100\n",
      "              ReLU-2           [-1, 10, 28, 28]               0\n",
      "            Conv2d-3           [-1, 10, 28, 28]             910\n",
      "              ReLU-4           [-1, 10, 28, 28]               0\n",
      "         MaxPool2d-5           [-1, 10, 14, 14]               0\n",
      "            Conv2d-6           [-1, 10, 14, 14]             910\n",
      "              ReLU-7           [-1, 10, 14, 14]               0\n",
      "            Conv2d-8           [-1, 10, 14, 14]             910\n",
      "              ReLU-9           [-1, 10, 14, 14]               0\n",
      "        MaxPool2d-10             [-1, 10, 7, 7]               0\n",
      "          Flatten-11                  [-1, 490]               0\n",
      "           Linear-12                   [-1, 10]           4,910\n",
      "================================================================\n",
      "Total params: 7,740\n",
      "Trainable params: 7,740\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.32\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "img = test_data[0][0].unsqueeze(0)\n",
    "y = test_data.targets[100]\n",
    "plt.imshow(img.squeeze(0).squeeze(0), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "NUM = 132\n",
    "\n",
    "img = test_data[NUM][0].unsqueeze(0)\n",
    "y = test_data.targets[100]\n",
    "\n",
    "target_layer = model.block_2[-3]\n",
    "\n",
    "activations = []\n",
    "gradients = []\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    gradients.append(grad_output[0])\n",
    "\n",
    "target_layer.register_forward_hook(forward_hook)\n",
    "target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "output = model(img)\n",
    "pred_class = output.argmax(dim=1).item()\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "output[:, pred_class].backward()\n",
    "\n",
    "gradients = gradients[0].cpu().data.numpy()\n",
    "activations = activations[0].cpu().data.numpy()\n",
    "\n",
    "weights = np.mean(gradients, axis=(2, 3))  # Global average pooling of gradients\n",
    "grad_cam = np.zeros(activations.shape[2:], dtype=np.float32)\n",
    "\n",
    "for i, w in enumerate(weights[0]):\n",
    "    grad_cam += w * activations[0, i, :, :]\n",
    "\n",
    "grad_cam = np.maximum(grad_cam, 0)\n",
    "\n",
    "grad_cam = (grad_cam - grad_cam.min()) / (grad_cam.max() - grad_cam.min())\n",
    "\n",
    "heatmap = cv2.resize(grad_cam, (img.shape[2], img.shape[3]))\n",
    "\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "\n",
    "\n",
    "img_np = img.cpu().squeeze().numpy() \n",
    "\n",
    "overlayed_img = heatmap * 0.4 + np.stack([img_np] * 3, axis=-1) * 0.6  # Replicating the grayscale image into 3 channels\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "fig.set_size_inches(15, 5)\n",
    "\n",
    "ax[0].imshow(heatmap)\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(np.clip(overlayed_img, 0, 1))\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(img.squeeze())\n",
    "ax[2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
