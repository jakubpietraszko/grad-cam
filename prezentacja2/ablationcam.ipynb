{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.56\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 384.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([1, 3, 224, 224]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_image(path, transform):\n",
    "    '''\n",
    "    get image from path and transform it tensor\n",
    "    '''\n",
    "    img = PIL.Image.open(path)\n",
    "    img_t = transform(img)\n",
    "    img_t = img_t.unsqueeze(0)\n",
    "    img_it = transform(img)\n",
    "    return img_t\n",
    "img = get_image('images/husky.jpg', transform)\n",
    "type(img), img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(img)\n",
    "pred = torch.nn.functional.softmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005702819209545851"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.argmax(pred).item()\n",
    "y_c = pred[0, c].item()\n",
    "y_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = model.layer4[-1].conv3\n",
    "\n",
    "weights = np.zeros((2048, 7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = None\n",
    "\n",
    "def hook_function(module, input, output):\n",
    "    global feature_maps\n",
    "    feature_maps = output.clone()\n",
    "\n",
    "last_conv_layer = model.layer4[-1].conv3\n",
    "hook = last_conv_layer.register_forward_hook(hook_function)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "output = model(img)\n",
    "output = torch.nn.functional.softmax(output, dim=1)\n",
    "y_c = output[0, c].item()\n",
    "argmax = torch.argmax(output).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ResNet.forward() got an unexpected keyword argument 'feature_maps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m ablated_feature_maps[:, k, :, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     ablated_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_maps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mablated_feature_maps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     y_c_k \u001b[38;5;241m=\u001b[39m ablated_output[\u001b[38;5;241m0\u001b[39m, c]\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ResNet.forward() got an unexpected keyword argument 'feature_maps'"
     ]
    }
   ],
   "source": [
    "for k in range(1):\n",
    "    ablated_feature_maps = feature_maps.clone()\n",
    "    ablated_feature_maps[:, k, :, :] = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ablated_output = model(img, feature_maps=ablated_feature_maps)\n",
    "        y_c_k = ablated_output[0, c].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jakub/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load a pre-trained ResNet model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Specify the target layer\n",
    "target_layer = model.layer4[-1].conv3\n",
    "\n",
    "# Preprocess the input image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def get_class_activation(model, input_image, class_index):\n",
    "    # Forward pass through the model to get the logits\n",
    "    output = model(input_image)\n",
    "    # Get the score for the target class\n",
    "    y_c = output[0, class_index]\n",
    "    return y_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(model, input_image, class_index, target_layer):\n",
    "    # Perform a forward pass with all feature maps intact to get y^c\n",
    "    y_c = get_class_activation(model, input_image, class_index)\n",
    "    \n",
    "    weights = []\n",
    "    \n",
    "    # Loop through each feature map in the target layer\n",
    "    for i, feature_map in enumerate(target_layer):\n",
    "        # Make a copy of the model\n",
    "        model_copy = model\n",
    "        # Set the feature map i in the target layer to zero\n",
    "        feature_map.zero_()\n",
    "        # Perform a forward pass to get y^c_k (after ablation)\n",
    "        y_c_k = get_class_activation(model_copy, input_image, class_index)\n",
    "        # Calculate the weight w_k^c\n",
    "        weight_k = (y_c - y_c_k) / y_c\n",
    "        weights.append(weight_k)\n",
    "        \n",
    "        # Restore the feature map to its original state\n",
    "        feature_map.detach()\n",
    "    \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load a pre-trained ResNet model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Specify the target layer\n",
    "target_layer = model.layer4[-1].conv3\n",
    "\n",
    "# Variable to store the feature maps\n",
    "activation = None\n",
    "\n",
    "# Hook function to capture the output of target_layer\n",
    "def hook_fn(module, input, output):\n",
    "    global activation\n",
    "    activation = output\n",
    "\n",
    "# Register the hook\n",
    "hook = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# Preprocess the input image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to get class activation score\n",
    "def get_class_activation(model, input_image, class_index):\n",
    "    # Forward pass through the model to get the logits\n",
    "    output = model(input_image)\n",
    "    # Get the score for the target class\n",
    "    y_c = output[0, class_index]\n",
    "    return y_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(model, input_image, class_index):\n",
    "    # Forward pass to get y^c and capture the activations\n",
    "    y_c = get_class_activation(model, input_image, class_index)\n",
    "    global activation\n",
    "    original_activation = activation.clone().detach()  # Store the original activations\n",
    "    \n",
    "    weights = []\n",
    "    # Loop through each feature map\n",
    "    for i in range(activation.shape[1]):  # Number of feature maps\n",
    "        print(i / activation.shape[1] * 100, '%')\n",
    "        # Ablate the i-th feature map by setting it to zero\n",
    "        activation[:, i, :, :] = 0\n",
    "        # Forward pass with the ablated feature map\n",
    "        y_c_k = get_class_activation(model, input_image, class_index)\n",
    "        \n",
    "        print(y_c_k)\n",
    "        # Calculate the weight w_k^c\n",
    "        weight_k = (y_c - y_c_k) / y_c\n",
    "        weights.append(weight_k.item())\n",
    "        \n",
    "        # Restore the feature map to its original state\n",
    "        activation[:, i, :, :] = original_activation[:, i, :, :]\n",
    "    \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.048828125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.09765625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.146484375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.1953125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.244140625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.29296875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.341796875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.390625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.439453125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.48828125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.537109375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.5859375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.634765625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.68359375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.732421875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.78125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.830078125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.87890625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.927734375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "0.9765625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.025390625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.07421875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.123046875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.171875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.220703125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.26953125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.318359375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.3671875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.416015625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.46484375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.513671875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.5625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.611328125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.66015625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.708984375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.7578125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.806640625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.85546875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.904296875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "1.953125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.001953125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.05078125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.099609375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.1484375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.197265625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.24609375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.294921875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.34375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.392578125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.44140625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.490234375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.5390625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.587890625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.63671875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.685546875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.734375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.783203125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.83203125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.880859375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.9296875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "2.978515625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.02734375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.076171875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.173828125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.22265625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.271484375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.3203125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.369140625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.41796875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.466796875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.515625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.564453125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.61328125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.662109375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.7109375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.759765625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.80859375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.857421875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.90625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "3.955078125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.00390625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.052734375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.1015625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.150390625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.19921875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.248046875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.296875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.345703125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.39453125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.443359375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.4921875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.541015625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.58984375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.638671875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.6875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.736328125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.78515625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.833984375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.8828125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.931640625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "4.98046875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.029296875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.078125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.126953125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.17578125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.224609375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.2734375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.322265625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.37109375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.419921875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.46875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.517578125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.56640625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.615234375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.6640625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.712890625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.76171875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.810546875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.859375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.908203125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "5.95703125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.005859375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.0546875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.103515625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.15234375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.201171875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.25 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.298828125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.34765625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.396484375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.4453125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.494140625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.54296875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.591796875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.640625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.689453125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.73828125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.787109375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.8359375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.884765625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.93359375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "6.982421875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.03125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.080078125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.12890625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.177734375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.2265625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.275390625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.32421875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.373046875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.421875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.470703125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.51953125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.568359375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.6171875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.666015625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.71484375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.763671875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.8125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.861328125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.91015625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "7.958984375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.0078125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.056640625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.10546875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.154296875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.203125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.251953125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.30078125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.349609375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.3984375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.447265625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.49609375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.544921875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.59375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.642578125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.69140625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.740234375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.7890625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.837890625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.88671875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.935546875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "8.984375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.033203125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.08203125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.130859375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.1796875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.228515625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.27734375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.326171875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.423828125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.47265625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.521484375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.5703125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.619140625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.66796875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.716796875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.765625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.814453125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.86328125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.912109375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "9.9609375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.009765625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.05859375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.107421875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.15625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.205078125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.25390625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.302734375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.3515625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.400390625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.44921875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.498046875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.546875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.595703125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.64453125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.693359375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.7421875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.791015625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.83984375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.888671875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.9375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "10.986328125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.03515625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.083984375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.1328125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.181640625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.23046875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.279296875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.328125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.376953125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.42578125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.474609375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.5234375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.572265625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.62109375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.669921875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.71875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.767578125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.81640625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.865234375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.9140625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "11.962890625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.01171875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.060546875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.109375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.158203125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.20703125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.255859375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.3046875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.353515625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.40234375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.451171875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.5 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.548828125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.59765625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.646484375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.6953125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.744140625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.79296875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.841796875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.890625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.939453125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "12.98828125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.037109375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.0859375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.134765625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.18359375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.232421875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.28125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.330078125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.37890625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.427734375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.4765625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.525390625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.57421875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.623046875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.671875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.720703125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.76953125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.818359375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.8671875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.916015625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "13.96484375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.013671875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.0625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.111328125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.16015625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.208984375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.2578125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.306640625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.35546875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.404296875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.453125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.501953125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.55078125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.599609375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.6484375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.697265625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.74609375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.794921875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.84375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.892578125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.94140625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "14.990234375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.0390625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.087890625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.13671875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.185546875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.234375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.283203125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.33203125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.380859375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.4296875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.478515625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.52734375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.576171875 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.673828125 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.72265625 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.771484375 %\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "15.8203125 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m class_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate weights for each feature map in the target layer\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(weights)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Remove the hook when done\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m, in \u001b[0;36mcalculate_weights\u001b[0;34m(model, input_image, class_index)\u001b[0m\n\u001b[1;32m     12\u001b[0m activation[:, i, :, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass with the ablated feature map\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m y_c_k \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_c_k)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate the weight w_k^c\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m, in \u001b[0;36mget_class_activation\u001b[0;34m(model, input_image, class_index)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_class_activation\u001b[39m(model, input_image, class_index):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Forward pass through the model to get the logits\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Get the score for the target class\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     y_c \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m, class_index]\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torchvision/models/resnet.py:146\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    144\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 146\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Tasks/final/grad-cam/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load an image\n",
    "image = Image.open('images/husky.jpg')\n",
    "input_image = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Choose a class index (e.g., 0 for the first class)\n",
    "class_index = 0\n",
    "\n",
    "# Calculate weights for each feature map in the target layer\n",
    "weights = calculate_weights(model, input_image, class_index)\n",
    "print(weights)\n",
    "\n",
    "# Remove the hook when done\n",
    "hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
